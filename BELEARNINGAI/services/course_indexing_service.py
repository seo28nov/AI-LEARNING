"""
Course Indexing Service - Index n·ªôi dung kh√≥a h·ªçc v√†o vector database.

Service n√†y ch·ªãu tr√°ch nhi·ªám:
1. Extract text t·ª´ course content (title, description, chapters, lessons)
2. Chunk text th√†nh c√°c ƒëo·∫°n nh·ªè (v·ªõi overlap ƒë·ªÉ gi·ªØ context)
3. Generate embeddings cho m·ªói chunk
4. Store embeddings v√†o vector database
5. Update/reindex khi course thay ƒë·ªïi
"""
from typing import List, Dict, Optional, Any
import logging
import re
from datetime import datetime

from beanie import PydanticObjectId
from models.models import CourseDocument
from services.embedding_service import embedding_service
from services.vector_service import vector_service

logger = logging.getLogger(__name__)


class CourseIndexingService:
    """Service ƒë·ªÉ index course content v√†o vector database."""
    
    # Constants
    CHUNK_SIZE = 500  # S·ªë t·ª´ m·ªói chunk
    CHUNK_OVERLAP = 100  # S·ªë t·ª´ overlap gi·ªØa c√°c chunks
    NAMESPACE = "courses"  # Namespace trong vector DB
    
    def __init__(self):
        """Kh·ªüi t·∫°o Course Indexing Service."""
        logger.info("‚úÖ Course Indexing Service ƒë√£ s·∫µn s√†ng")
    
    def _clean_text(self, text: str) -> str:
        """
        Clean v√† normalize text.
        
        Args:
            text: Raw text
            
        Returns:
            Cleaned text
        """
        if not text:
            return ""
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters (gi·ªØ l·∫°i d·∫•u c√¢u c∆° b·∫£n)
        text = re.sub(r'[^\w\s.,!?;:()\-]', '', text)
        
        # Strip
        text = text.strip()
        
        return text
    
    def _split_into_chunks(self, text: str, chunk_size: int = None, overlap: int = None) -> List[str]:
        """
        Chia text th√†nh c√°c chunks v·ªõi overlap.
        
        Args:
            text: Text c·∫ßn chia
            chunk_size: S·ªë t·ª´ m·ªói chunk (default: CHUNK_SIZE)
            overlap: S·ªë t·ª´ overlap (default: CHUNK_OVERLAP)
            
        Returns:
            List of text chunks
            
        Example:
            >>> text = "Python is a programming language. It is easy to learn."
            >>> chunks = self._split_into_chunks(text, chunk_size=5, overlap=2)
        """
        if not text:
            return []
        
        chunk_size = chunk_size or self.CHUNK_SIZE
        overlap = overlap or self.CHUNK_OVERLAP
        
        # Split th√†nh words
        words = text.split()
        
        if len(words) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(words):
            # L·∫•y chunk
            end = start + chunk_size
            chunk_words = words[start:end]
            chunk_text = ' '.join(chunk_words)
            chunks.append(chunk_text)
            
            # Move start pointer (tr·ª´ overlap)
            start = end - overlap
            
            # N·∫øu c√≤n √≠t t·ª´ h∆°n overlap, l·∫•y h·∫øt
            if len(words) - end < overlap:
                break
        
        return chunks
    
    async def _extract_course_content(self, course: CourseDocument) -> List[Dict[str, Any]]:
        """
        Extract t·∫•t c·∫£ n·ªôi dung text t·ª´ course.
        
        Args:
            course: Course object
            
        Returns:
            List of content dicts v·ªõi fields:
                - text: N·ªôi dung text
                - title: Ti√™u ƒë·ªÅ c·ªßa chunk
                - course_id: ID c·ªßa course
                - chapter_id: ID c·ªßa chapter (n·∫øu c√≥)
                - lesson_id: ID c·ªßa lesson (n·∫øu c√≥)
                - content_type: Lo·∫°i content (overview, chapter, lesson)
        """
        contents = []
        
        try:
            # 1. Course overview (title + description)
            overview_text = f"{course.title}\n\n{course.description or ''}"
            overview_text = self._clean_text(overview_text)
            
            if overview_text:
                contents.append({
                    "text": overview_text,
                    "title": course.title,
                    "course_id": str(course.id),
                    "chapter_id": None,
                    "lesson_id": None,
                    "content_type": "overview"
                })
            
            # 2. Chapters v√† Lessons
            if course.chapters:
                for chapter in course.chapters:
                    # Chapter content
                    chapter_text = f"{chapter.title}\n\n{chapter.description or ''}"
                    chapter_text = self._clean_text(chapter_text)
                    
                    if chapter_text:
                        contents.append({
                            "text": chapter_text,
                            "title": f"{course.title} - {chapter.title}",
                            "course_id": str(course.id),
                            "chapter_id": str(chapter.id),
                            "lesson_id": None,
                            "content_type": "chapter"
                        })
                    
                    # Lessons trong chapter
                    if chapter.lessons:
                        for lesson in chapter.lessons:
                            lesson_text = f"{lesson.title}\n\n{lesson.content or ''}"
                            lesson_text = self._clean_text(lesson_text)
                            
                            if lesson_text:
                                contents.append({
                                    "text": lesson_text,
                                    "title": f"{course.title} - {chapter.title} - {lesson.title}",
                                    "course_id": str(course.id),
                                    "chapter_id": str(chapter.id),
                                    "lesson_id": str(lesson.id),
                                    "content_type": "lesson"
                                })
            
            logger.info(f"üìÑ Extracted {len(contents)} content pieces from course {course.id}")
            
            return contents
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract course content: {e}")
            return []
    
    async def _chunk_content(self, contents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Chia c√°c content th√†nh chunks nh·ªè h∆°n.
        
        Args:
            contents: List of content dicts
            
        Returns:
            List of chunked content dicts
        """
        chunked_contents = []
        
        for content in contents:
            text = content["text"]
            
            # Split into chunks
            chunks = self._split_into_chunks(text)
            
            # N·∫øu ch·ªâ c√≥ 1 chunk, gi·ªØ nguy√™n
            if len(chunks) == 1:
                chunked_contents.append(content)
            else:
                # T·∫°o multiple chunks
                for i, chunk_text in enumerate(chunks):
                    chunk_dict = content.copy()
                    chunk_dict["text"] = chunk_text
                    chunk_dict["chunk_index"] = i
                    chunk_dict["total_chunks"] = len(chunks)
                    chunked_contents.append(chunk_dict)
        
        logger.info(f"üìä Chunked into {len(chunked_contents)} total chunks")
        
        return chunked_contents
    
    async def index_course(self, course: CourseDocument) -> Dict[str, Any]:
        """
        Index m·ªôt course v√†o vector database.
        
        Workflow:
        1. Extract content t·ª´ course
        2. Chunk content th√†nh c√°c ƒëo·∫°n nh·ªè
        3. Generate embeddings cho m·ªói chunk
        4. Store v√†o vector database
        
        Args:
            course: Course object c·∫ßn index
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£:
                - success: True/False
                - course_id: ID c·ªßa course
                - chunks_indexed: S·ªë chunks ƒë√£ index
                - message: Th√¥ng b√°o
        """
        try:
            logger.info(f"üöÄ B·∫Øt ƒë·∫ßu index course: {course.id} - {course.title}")
            
            # 1. Extract content
            contents = await self._extract_course_content(course)
            
            if not contents:
                logger.warning(f"‚ö†Ô∏è Course {course.id} kh√¥ng c√≥ content ƒë·ªÉ index")
                return {
                    "success": False,
                    "course_id": str(course.id),
                    "chunks_indexed": 0,
                    "message": "No content to index"
                }
            
            # 2. Chunk content
            chunked_contents = await self._chunk_content(contents)
            
            # 3. Generate embeddings
            texts = [c["text"] for c in chunked_contents]
            embeddings = await embedding_service.generate_embeddings_batch(
                texts,
                task_type="retrieval_document"
            )
            
            # 4. Prepare vectors cho vector database
            vectors = []
            for i, (chunk, embedding) in enumerate(zip(chunked_contents, embeddings)):
                vector = {
                    "id": f"{course.id}_chunk_{i}",
                    "values": embedding,
                    "metadata": {
                        "course_id": chunk["course_id"],
                        "chapter_id": chunk.get("chapter_id"),
                        "lesson_id": chunk.get("lesson_id"),
                        "content_type": chunk["content_type"],
                        "title": chunk["title"],
                        "text": chunk["text"][:1000],  # Gi·ªõi h·∫°n text trong metadata
                        "chunk_index": chunk.get("chunk_index", 0),
                        "indexed_at": datetime.utcnow().isoformat()
                    }
                }
                vectors.append(vector)
            
            # 5. Upsert v√†o vector database
            upsert_result = await vector_service.upsert_vectors(vectors, namespace=self.NAMESPACE)
            
            # Log k·∫øt qu·∫£ upsert ƒë·ªÉ debug
            logger.info(f"‚úÖ ƒê√£ index {len(vectors)} chunks cho course {course.id}")
            logger.debug(f"Upsert result: {upsert_result}")
            
            return {
                "success": True,
                "course_id": str(course.id),
                "chunks_indexed": len(vectors),
                "message": "Course indexed successfully"
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói index course {course.id}: {e}")
            return {
                "success": False,
                "course_id": str(course.id),
                "chunks_indexed": 0,
                "message": f"Error: {str(e)}"
            }
    
    async def reindex_course(self, course_id: PydanticObjectId) -> Dict[str, Any]:
        """
        Reindex m·ªôt course (x√≥a index c≈© v√† t·∫°o l·∫°i).
        
        Args:
            course_id: ID c·ªßa course
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£
        """
        try:
            logger.info(f"üîÑ Reindex course: {course_id}")
            
            # 1. X√≥a index c≈©
            await self.delete_course_index(course_id)
            
            # 2. Load course t·ª´ database
            course = await CourseDocument.get(course_id)
            
            if not course:
                logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y course {course_id}")
                return {
                    "success": False,
                    "course_id": str(course_id),
                    "message": "Course not found"
                }
            
            # 3. Index l·∫°i
            result = await self.index_course(course)
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói reindex course {course_id}: {e}")
            return {
                "success": False,
                "course_id": str(course_id),
                "message": f"Error: {str(e)}"
            }
    
    async def delete_course_index(self, course_id: PydanticObjectId) -> Dict[str, Any]:
        """
        X√≥a index c·ªßa m·ªôt course kh·ªèi vector database.
        
        Args:
            course_id: ID c·ªßa course
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£
        """
        try:
            logger.info(f"üóëÔ∏è X√≥a index c·ªßa course: {course_id}")
            
            # Delete by filter
            delete_result = await vector_service.delete_by_filter(
                filter_dict={"course_id": str(course_id)},
                namespace=self.NAMESPACE
            )
            
            logger.info(f"‚úÖ ƒê√£ x√≥a index c·ªßa course {course_id}")
            logger.debug(f"Delete result: {delete_result}")
            
            return {
                "success": True,
                "course_id": str(course_id),
                "message": "Course index deleted"
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x√≥a course index {course_id}: {e}")
            return {
                "success": False,
                "course_id": str(course_id),
                "message": f"Error: {str(e)}"
            }
    
    async def search_course_content(
        self,
        query: str,
        course_id: Optional[str] = None,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Search content trong courses.
        
        Args:
            query: Query string t·ª´ user
            course_id: Optional - ch·ªâ search trong course n√†y
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            List of search results v·ªõi:
                - text: N·ªôi dung
                - title: Ti√™u ƒë·ªÅ
                - course_id: ID course
                - score: Similarity score
                
        Example:
            >>> results = await indexing_service.search_course_content(
            ...     "Python list comprehension",
            ...     top_k=3
            ... )
        """
        try:
            logger.info(f"üîç Search: '{query}' (top_k={top_k})")
            
            # 1. Generate query embedding
            query_embedding = await embedding_service.generate_query_embedding(query)
            
            # 2. Build filter
            filter_dict = None
            if course_id:
                filter_dict = {"course_id": course_id}
            
            # 3. Search trong vector database
            results = await vector_service.search(
                query_vector=query_embedding,
                top_k=top_k,
                namespace=self.NAMESPACE,
                filter_dict=filter_dict
            )
            
            # 4. Format results
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "text": result["metadata"].get("text", ""),
                    "title": result["metadata"].get("title", ""),
                    "course_id": result["metadata"].get("course_id"),
                    "chapter_id": result["metadata"].get("chapter_id"),
                    "lesson_id": result["metadata"].get("lesson_id"),
                    "content_type": result["metadata"].get("content_type"),
                    "score": result.get("score", 0.0)
                })
            
            logger.info(f"‚úÖ T√¨m th·∫•y {len(formatted_results)} k·∫øt qu·∫£")
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói search content: {e}")
            return []
    
    async def index_all_courses(self) -> Dict[str, Any]:
        """
        Index t·∫•t c·∫£ courses trong database.
        
        D√πng cho initial setup ho·∫∑c bulk reindex.
        
        Returns:
            Dict v·ªõi th·ªëng k√™:
                - total_courses: T·ªïng s·ªë courses
                - indexed: S·ªë courses ƒë√£ index th√†nh c√¥ng
                - failed: S·ªë courses b·ªã l·ªói
                - total_chunks: T·ªïng s·ªë chunks ƒë√£ index
        """
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu index t·∫•t c·∫£ courses...")
            
            # Load t·∫•t c·∫£ courses
            courses = await CourseDocument.find_all().to_list()
            
            total_courses = len(courses)
            indexed_count = 0
            failed_count = 0
            total_chunks = 0
            
            for course in courses:
                result = await self.index_course(course)
                
                if result["success"]:
                    indexed_count += 1
                    total_chunks += result["chunks_indexed"]
                else:
                    failed_count += 1
            
            logger.info(f"‚úÖ Ho√†n th√†nh index: {indexed_count}/{total_courses} courses")
            
            return {
                "total_courses": total_courses,
                "indexed": indexed_count,
                "failed": failed_count,
                "total_chunks": total_chunks
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói index all courses: {e}")
            return {
                "total_courses": 0,
                "indexed": 0,
                "failed": 0,
                "total_chunks": 0,
                "error": str(e)
            }


# ============================================================================
# SINGLETON INSTANCE
# ============================================================================

# Kh·ªüi t·∫°o course indexing service duy nh·∫•t
course_indexing_service = CourseIndexingService()
logger.info("‚úÖ Course Indexing Service singleton ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")
