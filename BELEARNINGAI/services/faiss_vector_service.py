"""
FAISS Vector Service - Thay th·∫ø ChromaDB v·ªõi Facebook AI Similarity Search.

FAISS ƒë∆∞·ª£c ch·ªçn l√†m vector database cho BeLearning v√¨:
‚úÖ No compile required - c√≥ s·∫µn pre-built wheels cho Windows
‚úÖ Facebook AI production-ready - ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i
‚úÖ Extremely fast - nhanh h∆°n ChromaDB ƒë√°ng k·ªÉ  
‚úÖ Local storage - kh√¥ng ph·ª• thu·ªôc cloud, data stays on-premise
‚úÖ Free & open source - kh√¥ng c√≥ chi ph√≠ ƒë·ªãnh k·ª≥
‚úÖ Memory efficient - t·ªëi ∆∞u cho large-scale datasets
‚úÖ Multiple index types - h·ªó tr·ª£ nhi·ªÅu thu·∫≠t to√°n t√¨m ki·∫øm
‚úÖ Python native - integration m∆∞·ª£t m√† v·ªõi FastAPI

FAISS ph√π h·ª£p v·ªõi educational platform v√¨:
- Scale excellent cho h√†ng ngh√¨n courses
- Performance v∆∞·ª£t tr·ªôi cho real-time search
- Cost-effective cho institutions
- Self-hosted - full control over data
"""
import logging
import pickle
import json
import os
from typing import List, Dict, Any, Optional
import numpy as np

try:
    import faiss
except ImportError:
    faiss = None

from config.config import settings

logger = logging.getLogger(__name__)


class FAISSVectorService:
    """
    Vector database service s·ª≠ d·ª•ng FAISS.
    
    FAISS l∆∞u tr·ªØ embeddings trong memory v√† tr√™n disk, cung c·∫•p
    similarity search c·ª±c nhanh v·ªõi cosine similarity ho·∫∑c L2 distance.
    """
    
    def __init__(self):
        """
        Kh·ªüi t·∫°o FAISS vector service.
        
        S·ª≠ d·ª•ng persistent storage ƒë·ªÉ data kh√¥ng m·∫•t khi restart.
        """
        if faiss is None:
            raise ImportError("FAISS is not installed. Please install: pip install faiss-cpu")
            
        try:
            # L·∫•y persist directory t·ª´ settings
            self.persist_directory = getattr(settings, "vector_persist_directory", "./faiss_db")
            
            # T·∫°o directory n·∫øu ch∆∞a c√≥
            os.makedirs(self.persist_directory, exist_ok=True)
            
            logger.info(f"üîß Initializing FAISS with persist_directory: {self.persist_directory}")
            
            # Dictionary ƒë·ªÉ l∆∞u c√°c collections (namespaces)
            self.collections = {}
            
            # Load existing collections
            self._load_collections()
            
            logger.info("‚úÖ FAISS Vector Service initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize FAISS: {e}")
            raise
    
    def _get_collection_path(self, namespace: str) -> Dict[str, str]:
        """L·∫•y file paths cho collection."""
        safe_name = namespace.replace("/", "_").replace(" ", "_")
        return {
            "index": os.path.join(self.persist_directory, f"{safe_name}.index"),
            "metadata": os.path.join(self.persist_directory, f"{safe_name}.metadata"),
            "ids": os.path.join(self.persist_directory, f"{safe_name}.ids")
        }
    
    def _load_collections(self):
        """Load t·∫•t c·∫£ collections t·ª´ disk."""
        try:
            for file in os.listdir(self.persist_directory):
                if file.endswith(".index"):
                    namespace = file[:-6]  # Remove .index extension
                    self._load_collection(namespace)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error loading collections: {e}")
    
    def _load_collection(self, namespace: str):
        """Load m·ªôt collection t·ª´ disk."""
        try:
            paths = self._get_collection_path(namespace)
            
            if os.path.exists(paths["index"]):
                # Load FAISS index
                index = faiss.read_index(paths["index"])
                
                # Load metadata
                metadata = {}
                if os.path.exists(paths["metadata"]):
                    with open(paths["metadata"], 'rb') as f:
                        metadata = pickle.load(f)
                
                # Load IDs
                ids = []
                if os.path.exists(paths["ids"]):
                    with open(paths["ids"], 'r', encoding='utf-8') as f:
                        ids = json.load(f)
                
                self.collections[namespace] = {
                    "index": index,
                    "metadata": metadata,
                    "ids": ids,
                    "dimension": index.d
                }
                
                logger.info(f"‚úÖ Loaded collection '{namespace}' with {len(ids)} vectors")
        except Exception as e:
            logger.error(f"‚ùå Error loading collection '{namespace}': {e}")
    
    def _save_collection(self, namespace: str):
        """Save m·ªôt collection xu·ªëng disk."""
        try:
            if namespace not in self.collections:
                return
                
            paths = self._get_collection_path(namespace)
            collection = self.collections[namespace]
            
            # Save FAISS index
            faiss.write_index(collection["index"], paths["index"])
            
            # Save metadata
            with open(paths["metadata"], 'wb') as f:
                pickle.dump(collection["metadata"], f)
            
            # Save IDs
            with open(paths["ids"], 'w', encoding='utf-8') as f:
                json.dump(collection["ids"], f, ensure_ascii=False, indent=2)
                
            logger.debug(f"üíæ Saved collection '{namespace}'")
        except Exception as e:
            logger.error(f"‚ùå Error saving collection '{namespace}': {e}")
    
    def _get_or_create_collection(self, namespace: str, dimension: int = 768):
        """
        Get ho·∫∑c create collection trong FAISS.
        
        Args:
            namespace: T√™n collection
            dimension: Dimension c·ªßa embedding vectors
            
        Returns:
            Collection dict v·ªõi index, metadata, ids
        """
        try:
            if namespace in self.collections:
                return self.collections[namespace]
            
            # Create new collection
            # S·ª≠ d·ª•ng IndexFlatIP cho cosine similarity (Inner Product)
            # Note: V·ªõi normalized vectors, IP = cosine similarity
            index = faiss.IndexFlatIP(dimension)
            
            self.collections[namespace] = {
                "index": index,
                "metadata": {},  # Dict[id -> metadata]
                "ids": [],  # List of IDs theo th·ª© t·ª± trong index
                "dimension": dimension
            }
            
            logger.info(f"‚úÖ Created new collection '{namespace}' with dimension {dimension}")
            
            return self.collections[namespace]
            
        except Exception as e:
            logger.error(f"‚ùå Error getting/creating collection '{namespace}': {e}")
            raise
    
    def _normalize_vector(self, vector: List[float]) -> np.ndarray:
        """Normalize vector ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi cosine similarity."""
        vec = np.array(vector, dtype=np.float32)
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec = vec / norm
        return vec.reshape(1, -1)
    
    async def upsert_vectors(
        self,
        vectors: List[Dict[str, Any]],
        namespace: str = "courses"
    ) -> Dict[str, Any]:
        """
        Insert ho·∫∑c update vectors v√†o FAISS.
        
        Args:
            vectors: List of dicts v·ªõi format gi·ªëng ChromaDB
            namespace: Collection name
            
        Returns:
            Dict v·ªõi th√¥ng tin upsert
        """
        try:
            if not vectors:
                logger.warning("‚ö†Ô∏è No vectors to upsert")
                return {
                    "success": False,
                    "upserted_count": 0,
                    "namespace": namespace,
                    "error": "No vectors provided"
                }
            
            logger.info(f"üìä Upserting {len(vectors)} vectors to namespace '{namespace}'")
            
            # Get dimension t·ª´ vector ƒë·∫ßu ti√™n
            dimension = len(vectors[0]["values"])
            
            # Get ho·∫∑c create collection
            collection = self._get_or_create_collection(namespace, dimension)
            
            # Prepare data
            new_vectors = []
            new_ids = []
            new_metadata = {}
            
            for vector in vectors:
                vector_id = vector["id"]
                values = vector["values"]
                metadata = vector.get("metadata", {})
                
                # Check n·∫øu ID ƒë√£ t·ªìn t·∫°i - n·∫øu c√≥ th√¨ update
                if vector_id in collection["ids"]:
                    # Update existing vector
                    # FAISS kh√¥ng h·ªó tr·ª£ update tr·ª±c ti·∫øp, ch·ªâ update metadata
                    collection["metadata"][vector_id] = metadata
                    # Note: Vector values kh√¥ng th·ªÉ update, ph·∫£i delete r·ªìi add l·∫°i
                    # ho·∫∑c rebuild to√†n b·ªô index. Hi·ªán t·∫°i ch·ªâ update metadata.
                else:
                    # Add new vector
                    normalized_vec = self._normalize_vector(values)
                    new_vectors.append(normalized_vec[0])
                    new_ids.append(vector_id)
                    new_metadata[vector_id] = metadata
            
            # Add new vectors to index
            if new_vectors:
                vectors_array = np.array(new_vectors, dtype=np.float32)
                collection["index"].add(vectors_array)
                
                # Update IDs v√† metadata
                collection["ids"].extend(new_ids)
                collection["metadata"].update(new_metadata)
            
            # Save to disk
            self._save_collection(namespace)
            
            logger.info(f"‚úÖ Successfully upserted {len(vectors)} vectors")
            
            return {
                "success": True,
                "upserted_count": len(vectors),
                "namespace": namespace
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error upserting vectors: {e}")
            return {
                "success": False,
                "upserted_count": 0,
                "namespace": namespace,
                "error": str(e)
            }
    
    async def search(
        self,
        query_vector: List[float],
        top_k: int = 5,
        namespace: str = "courses",
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Search vectors t∆∞∆°ng t·ª± trong FAISS.
        
        Args:
            query_vector: Query embedding vector
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
            namespace: Collection name
            filter_dict: Metadata filters (s·∫Ω filter sau khi search)
            
        Returns:
            List of results v·ªõi format gi·ªëng ChromaDB
        """
        try:
            logger.info(f"üîç Searching in namespace '{namespace}' with top_k={top_k}")
            
            if namespace not in self.collections:
                logger.warning(f"‚ö†Ô∏è Collection '{namespace}' not found")
                return []
            
            collection = self.collections[namespace]
            
            if len(collection["ids"]) == 0:
                logger.warning(f"‚ö†Ô∏è Collection '{namespace}' is empty")
                return []
            
            # Normalize query vector
            query_vec = self._normalize_vector(query_vector)
            
            # Search trong FAISS index
            # L·∫•y nhi·ªÅu h∆°n top_k ƒë·ªÉ c√≥ th·ªÉ filter
            search_k = min(top_k * 3, len(collection["ids"]))
            scores, indices = collection["index"].search(query_vec, search_k)
            
            # Format results
            results = []
            
            for i, idx in enumerate(indices[0]):
                if idx == -1:  # FAISS tr·∫£ v·ªÅ -1 khi kh√¥ng t√¨m th·∫•y
                    continue
                
                vector_id = collection["ids"][idx]
                score = float(scores[0][i])  # Cosine similarity score
                metadata = collection["metadata"].get(vector_id, {})
                
                # Apply filter n·∫øu c√≥
                if filter_dict:
                    match = all(
                        metadata.get(key) == value
                        for key, value in filter_dict.items()
                    )
                    if not match:
                        continue
                
                results.append({
                    "id": vector_id,
                    "score": score,
                    "metadata": metadata
                })
                
                # D·ª´ng khi ƒë√£ c√≥ ƒë·ªß top_k results
                if len(results) >= top_k:
                    break
            
            logger.info(f"‚úÖ Found {len(results)} results")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error searching vectors: {e}")
            return []
    
    async def delete_by_filter(
        self,
        filter_dict: Dict[str, Any],
        namespace: str = "courses"
    ) -> Dict[str, Any]:
        """
        X√≥a vectors theo metadata filter.
        
        Note: FAISS kh√¥ng h·ªó tr·ª£ delete tr·ª±c ti·∫øp, ph·∫£i rebuild index.
        """
        try:
            logger.info(f"üóëÔ∏è Deleting vectors in '{namespace}' with filter: {filter_dict}")
            
            if namespace not in self.collections:
                return {
                    "success": True,
                    "deleted_count": 0,
                    "namespace": namespace
                }
            
            collection = self.collections[namespace]
            
            # Find IDs to delete
            ids_to_delete = []
            for vector_id in collection["ids"]:
                metadata = collection["metadata"].get(vector_id, {})
                match = all(
                    metadata.get(key) == value
                    for key, value in filter_dict.items()
                )
                if match:
                    ids_to_delete.append(vector_id)
            
            if not ids_to_delete:
                logger.info("‚ö†Ô∏è No vectors found matching filter")
                return {
                    "success": True,
                    "deleted_count": 0,
                    "namespace": namespace
                }
            
            # Rebuild index without deleted vectors
            remaining_vectors = []
            remaining_ids = []
            remaining_metadata = {}
            
            for i, vector_id in enumerate(collection["ids"]):
                if vector_id not in ids_to_delete:
                    # Get vector from index
                    vector = collection["index"].reconstruct(i)
                    remaining_vectors.append(vector)
                    remaining_ids.append(vector_id)
                    remaining_metadata[vector_id] = collection["metadata"][vector_id]
            
            # Create new index
            new_index = faiss.IndexFlatIP(collection["dimension"])
            if remaining_vectors:
                vectors_array = np.array(remaining_vectors, dtype=np.float32)
                new_index.add(vectors_array)
            
            # Update collection
            collection["index"] = new_index
            collection["ids"] = remaining_ids
            collection["metadata"] = remaining_metadata
            
            # Save
            self._save_collection(namespace)
            
            logger.info(f"‚úÖ Deleted {len(ids_to_delete)} vectors")
            
            return {
                "success": True,
                "deleted_count": len(ids_to_delete),
                "namespace": namespace
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error deleting vectors: {e}")
            return {
                "success": False,
                "deleted_count": 0,
                "namespace": namespace,
                "error": str(e)
            }
    
    async def get_collection_stats(self, namespace: str = "courses") -> Dict[str, Any]:
        """
        L·∫•y th·ªëng k√™ v·ªÅ collection.
        """
        try:
            if namespace not in self.collections:
                return {
                    "name": namespace,
                    "count": 0,
                    "error": "Collection not found"
                }
            
            collection = self.collections[namespace]
            
            return {
                "name": namespace,
                "count": len(collection["ids"]),
                "dimension": collection["dimension"],
                "metadata": {
                    "index_type": "IndexFlatIP",
                    "similarity_metric": "cosine"
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error getting collection stats: {e}")
            return {
                "name": namespace,
                "count": 0,
                "error": str(e)
            }
    
    async def reset_collection(self, namespace: str = "courses") -> Dict[str, Any]:
        """
        X√≥a to√†n b·ªô collection (d√πng cho testing/reset).
        """
        try:
            logger.warning(f"‚ö†Ô∏è Resetting collection '{namespace}'")
            
            if namespace in self.collections:
                del self.collections[namespace]
            
            # Delete files
            paths = self._get_collection_path(namespace)
            for path in paths.values():
                if os.path.exists(path):
                    os.remove(path)
            
            logger.info(f"‚úÖ Collection '{namespace}' reset successfully")
            
            return {
                "success": True,
                "message": f"Collection '{namespace}' deleted"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error resetting collection: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def __del__(self):
        """Cleanup khi service b·ªã destroy."""
        try:
            # Save t·∫•t c·∫£ collections
            for namespace in self.collections:
                self._save_collection(namespace)
        except Exception as e:
            # Log l·ªói ƒë·ªÉ debug, kh√¥ng silent fail
            logger.error(f"‚ùå Failed to save collections on shutdown: {e}")


# ============================================================================
# SINGLETON INSTANCE  
# ============================================================================

# Kh·ªüi t·∫°o FAISS vector service
try:
    vector_service = FAISSVectorService()
    logger.info("‚úÖ FAISS Vector Service singleton initialized")
except Exception as e:
    logger.error(f"‚ùå Cannot initialize FAISS Vector Service: {e}")
    vector_service = None